---
title: "Week2399"
date: 2023-09-12T15:57:03+08:00
draft: true
---

---
Q. now the example is selecting from mocked data, I want to proceed with real data in our s3/ hive/ presto

Sure, you can definitely use PySpark to query data stored in S3, Hive, or Presto. Here are some examples of how you can do this:

**1. Querying data from S3:**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName('S3 example') \
    .getOrCreate()

df = spark.read.csv("s3a://your_bucket/your_file.csv")
df.createOrReplaceTempView("my_table")

result = spark.sql("SELECT * FROM my_table")
```

**2. Querying data from Hive:**

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName('Hive example') \
    .enableHiveSupport() \
    .getOrCreate()

result = spark.sql("SELECT * FROM hive_table")
```

**3. Querying data from Presto:**

For Presto, you would typically use the `pyhive` library, not PySpark, to execute your queries:

```python
from pyhive import presto
import pandas as pd

cursor = presto.connect('localhost').cursor()
cursor.execute('SELECT * FROM presto_table')

# Fetch result into DataFrame
result = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])
```

Please replace `"s3a://your_bucket/your_file.csv"`, `"hive_table"`, and `"presto_table"` with your actual S3 bucket path, Hive table name, and Presto table name, respectively.

Remember, do not expose any confidential or sensitive data during this discussion.